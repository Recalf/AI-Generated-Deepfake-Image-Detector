---
license: mit
language:
- en
metrics:
- accuracy
- f1
- roc_auc
base_model:
- timm/convnextv2_base.fcmae_ft_in1k
pipeline_tag: image-classification
tags:
- ai
- detector
- deepfake
---


# Summary
This project detects whether an image is real or AI-Generated / AI-Deepfake using **ConvNeXtV2-Base (256×256)**, pretrained on ImageNet-1K.<br>
The model was trained in two phases: first on ~400k AI/Real images to build a strong foundation, then with a smaller continual learning cycle to adapt to the latest generative AI models.

The model detects images generated by many state-of-the-art (SOTA) generative models, including **DALL-E3, Flux, Nano Banana Pro, Diffusion Models (SDXL, SD3.5...), MidjourneyV6** and more, classifying them as fake.

# Test Score (OOD):
EvalGen (SOTA generative models): **90.40% Accuracy**

# Training Summary:
Training follows techniques from deep learning research literature, such as Layer-wise Learning Rate Decay **(LLRD)** to protect early layers during fine-tuning,<br>
**Cosine Annealing** with a 5-7% **Warmup** (LinearLR),<br>
**Rehearsal Buffer** (stratified datasets) for **Continual Learning**,<br>
Gradient Clipping, Label Smoothing...

Training/Testing are optimized with **Automatic Mixed Precision** (AMP) using Tensor Cores and other efficient **CUDA optimizations**<br>
(If run on a CPU, it falls back to non GPU; if the GPU doesn't support AMP, it falls back to FP32)<br>
**Data Augmentations** include a probability for **Random Resampling** & **Random Jpeg Reencoding** to simulate real world images.

**Side Note:** The LLRD/Cosine... recipe is originally for fine-tuning ViTs (MAE and BEiT research papers), but it work well on ConvNeXt even though its a CNN architecture,<br>
shortly, it’s because ConvNeXt was designed to act like ViTs while keeping CNN architecture efficiency.
# Project Structure
├── train/<br>
│ ├── train.py # Phase 1 training<br>
│ └── Continual Learning.py # Phase 2 continual learning<br>
├── test/<br>
│ └── test.py # Model evaluation<br>
├── model/<br>
│ └── convnext.py # ConvNeXtV2 model architecture<br>
├── checkpoints/<br>
│ └── checkpoint_phase1.pth<br>
│ └── checkpoint_phase2.pth<br>
├── inference.py # Streamlit web interface<br>
├── data_loaders.py # Dataset loading utilities<br>
├── engine.py # Training/testing loops<br>
└── transforms.py # Data augmentation pipelines<br>

# Training Setup:
**Phase 1:** <br>
400k images from 11 different datasets, 8 epochs with batch-size 30 using:<br>
AdamW(wd 0.02),<br>
LLRD(headlr: 2e-4, lr_decay:0.8, filtering Bias/BN on),<br>
Cosine Annealing (eta_min 0) with 5% Warmup (linearLR, lr * 0.01),<br>
Label Smoothing (0.05),<br>
Gradient Clipping

**Phase 2:** <br>
Continual Learning (i used 20k images), 5 epochs with batch-size 28 using:<br>
Rehearsal Buffer (stratified replay 1:8),<br>
AdamW(wd 0.02),<br>
LLRD(headlr: 7.5e-5, lr_decay:0.85, filtering Bias/BN on),<br>
Cosine Annealing (eta_min 1e-7) with 7% Warmup (linearLR, lr * 0.01),<br>
Label Smoothing (0.05),<br>
Gradient Clipping<br>

(out of numerous phase 2 hyperparameters/setups i experimented with, this worked out the best for my 20k dataset, especially with that 1:8 replay)
# Datasets Used
**Training Datasets [Phase 1 - ~400k images] (I halved the number of samples in some large datasets):**
- DDA-Training-Set (COCO + SD2 generated pairs)&nbsp;&nbsp;&nbsp;[*huggingface.co/datasets/Junwei-Xi/DDA-Training-Set*]
- Defactify (MS COCOAI: SD21, SDXL, SD3, DALL-E3, MidjourneyV6)&nbsp;&nbsp;&nbsp;[*huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset*]
- VisCounter_COCOAI:&nbsp;&nbsp;&nbsp;[*huggingface.co/datasets/NasrinImp/COCO_AI*]
- genimage_tiny (Midjourney, BigGAN, VQDM, SDv5, Wukong, ADM, GLIDE)&nbsp;&nbsp;&nbsp;[*kaggle.com/datasets/yangsangtai/tiny-genimage*]
- art_artai&nbsp;&nbsp;&nbsp;[*kaggle.com/datasets/cashbowman/ai-generated-images-vs-real-images*]
- Midjourney_small&nbsp;&nbsp;&nbsp;[*kaggle.com/datasets/mariammarioma/midjourney-imagenet-real-vs-synth*]
- DF40 (Deepfake)&nbsp;&nbsp;&nbsp;[*github.com/YZY-stack/DF40*]
- Gravex200k&nbsp;&nbsp;&nbsp;[*kaggle.com/datasets/muhammadbilal6305/200k-real-vs-ai-visuals-by-mbilal*]
- StyleGan2&nbsp;&nbsp;&nbsp;[*kaggle.com/datasets/kshitizbhargava/deepfake-face-images*]
- human_faces_hass&nbsp;&nbsp;&nbsp;[*kaggle.com/datasets/hassnainzaidi/human-faces-data-set* ]
- dfk_oldmonk&nbsp;&nbsp;&nbsp;[*kaggle.com/datasets/saurabhbagchi/deepfake-image-detection*]

**Phase 2 Continual Learning [~20k new images]:**
- Super_GenAI_Dataset&nbsp;&nbsp;&nbsp;[*kaggle.com/datasets/hiddenplant/sut-project?select=Super_GenAI_Dataset*]
- midjourney-dalle-sd-nanobananapro-dataset&nbsp;&nbsp;&nbsp;[*huggingface.co/datasets/julienlucas/midjourney-dalle-sd-nanobananapro-dataset?utm_source=chatgpt.com*]

# Usage/How to Run
**Model Checkpoint: (Checkpoints are too large for GitHub)**
- Download my checkpoint ("checkpoint_phase2") from my [*huggingface.co/xRayon/convnext-ai-images-detector/tree/main/AI%20Images%20Detector/checkpoints*]<br>
- Place it inside my checkpoints/ folder.

**Inference (Web Interface):** 
- Open CMD in the project directory (where inference.py and requirements.txt are located).
- Install the requirements: pip install -r requirements.txt 
- Run: streamlit run inference.py

# Requirements
- Python 3.x
- PyTorch 2.7.1
- torchvision 0.22.1
- timm 1.0.24
- streamlit 1.54.0
- See `requirements.txt` for full list
